_target_: src.pl_modules.pl_former.TSForecastFormer

name: Autoformer

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0016
  betas: [0.9, 0.95]
  weight_decay: 0.05

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.5
  patience: 10

model:
  _target_: src.models.Autoformer.Model
  enc_in: 6 # Input size of encoder
  dec_in: 6 # Input size of decoder
  c_out: 1 # Output size
  seq_len: 48 # Length of input sequence
  label_len: 24 # Length of label sequence
  pred_len: 48 # Length of prediction sequence
  moving_avg: 25 # Moving average window size
  d_model: 512 # Dimension of the model
  n_heads: 8 # Number of heads
  e_layers: 3 # Number of encoder layers
  d_layers: 2 # Number of decoder layers
  d_ff: 2048 # Dimension of FCN
  factor: 5 # ProbSparse Attention factor
  dropout: 0.05 # Dropout probability
  embed: timeF # Type of time features encoding [timeF, fixed, learned]
  activation: gelu # Activation function 
  output_attention: False # Whether to output attention in the encoder

metrics:
  train:
    rmse:
      _target_: torchmetrics.regression.MeanSquaredError
      squared: False
    mae: 
      _target_: torchmetrics.regression.MeanSquaredError
    mape:
      _target_: torchmetrics.regression.MeanAbsolutePercentageError
  val:
    rmse:
      _target_: torchmetrics.regression.MeanSquaredError
      squared: False
    mae: 
      _target_: torchmetrics.regression.MeanSquaredError
    mape:
      _target_: torchmetrics.regression.MeanAbsolutePercentageError
criterion:
  _target_: torch.nn.MSELoss
monitor: val/rmse

# seq_len: 48
# label_len: 24
# pred_len: 48
# padding: 0
# inverse_scaling: False
# output_attention: False
# scaler: None
  